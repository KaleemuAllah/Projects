{"cells":[{"cell_type":"markdown","id":"9feb0b9e-d754-4ae9-85c3-1c11356425d4","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX0JV6EN1119-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"d26b6d98-d2a2-4eb0-8a8f-6a928ce1c862","metadata":{},"source":["# **Use AI to Draw Human Portrait**\n"]},{"cell_type":"markdown","id":"baf987cc-2f4b-47cc-95cd-69b2794f26a1","metadata":{},"source":["Estimated time needed: **60** minutes\n"]},{"cell_type":"markdown","id":"fa94fa2c-3a9d-4906-909a-93e81d05118d","metadata":{},"source":["You are a Data Scientist hired by a non-profit organization that recently started a campaign to help people with disabilities boost self-confidence. As part of the campaign, the organization will give people free portraits of themselves. Due to the high popularity of the campaign, it would be very costly to hire many artists to draw these portraits. Therefore, your job is to use artificial intelligence to automate human portrait drawing.  \n","\n","In this guided project, you will learn how the famous Portrait Drawing app works and build your own portrait drawing tool by studying the architecture of the state-of-the-art **U-squared Net** model. Take a glance at the model output below. It gives you the same results as the ones you would get from the app!\n"]},{"cell_type":"markdown","id":"7b91ee7d-a935-4eb6-8786-0752a3c08c45","metadata":{},"source":["<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0JV6EN/images/intro.png\" width=\"60%\"></center>\n"]},{"cell_type":"markdown","id":"66621941-1c8e-46a0-8ba0-35316074e444","metadata":{},"source":["<p style=\"color:gray; text-align:center;\">Image credits to <a href=\"https://github.com/xuebinqin/U-2-Net\">Xuebin Qin et al.</a></p>\n"]},{"cell_type":"markdown","id":"ac3bb842-ec10-4661-ae7f-0b9f1365d05f","metadata":{},"source":["## __Table of Contents__\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Background-(optional)\">Background (optional)</a>\n","        <ol>\n","            <li><a href=\"#What-does-a-RSU-block-consist-of?\">What does a RSU block consist of?</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Code the RSU Blocks\">Code the RSU Blocks</a>\n","        <ol>\n","            <li><a href=\"#RSU-7\">RSU-7</a></li>\n","            <li><a href=\"#RSU-6\">RSU-6</a></li>\n","            <li><a href=\"#RSU-5\">RSU-5</a></li>\n","            <li><a href=\"#Exercise: Create RSU-4\">Exercise: Create RSU-4</a></li>\n","            <li><a href=\"#RSU-4F\">RSU-4F</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#U2-Net Architecture: Two-level Nested U-structure\">U2-Net Architecture: Two-level Nested U-structure</a>\n","        <ol>\n","            <li><a href=\"#Configure U2-Net\">Configure U2-Net</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Use U2-Net to Draw Human Portrait\">Use U2-Net to Draw Human Portrait</a>\n","        <ol>\n","            <li><a href=\"#Training\">Training</a></li>\n","            <li><a href=\"#Pre-trained U-squared Net\">Pre-trained U-squared Net</a></li>\n","            <li><a href=\"#Creating a Dataloader\">Creating a Dataloader</a></li>\n","            <li><a href=\"#Inference\">Inference</a></li>\n","            <li><a href=\"#Exercise: Generate your own portrait\">Exercise: Generate your own portrait</a></li>           \n","        </ol>\n","    </li>\n","</ol>\n"]},{"cell_type":"markdown","id":"17e300fc-979e-4ed9-9382-0a4e8b5a1382","metadata":{},"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n"," - Understand the use and configuration of Residual U-blocks\n"," - Code Residual U-blocks with different depths in PyTorch\n"," - Describe the architecture of U2-Net\n"," - Construct U2-Net architecture using Residual U-blocks \n"," - Produce saliency probability maps of an input image as side outputs of U2-Net\n"," - Create PyTorch Dataset object and PyTorch DataLoader \n"," - Load the pre-trained weights of a U2-Net for inference\n"]},{"cell_type":"markdown","id":"fb648edd-e68d-42cd-b65f-b0797cfc8f25","metadata":{},"source":["----\n"]},{"cell_type":"markdown","id":"ded3f60f-b777-4e26-97d5-8edb6347c13f","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","id":"fabd1ec7-a96f-4ca6-a8ec-f581cfce2291","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","id":"90fc1ecd-2d8f-46f3-ab01-2506e73281f6","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","execution_count":1,"id":"04ce4a97-d697-4b61-b968-12aa864c1820","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"markdown","id":"255c8218-4e8d-446d-a7a2-dcaf50c9f772","metadata":{},"source":["### Importing Required Libraries\n","\n","_We recommend you import all required libraries in one place (here):_\n"]},{"cell_type":"code","execution_count":2,"id":"5ddcd588-0c63-4b92-b857-e7ab1cb7dbcd","metadata":{},"outputs":[],"source":["def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","warnings.filterwarnings('ignore')\n","\n","import skillsnetwork\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import os\n","import numpy as np\n","from skimage import io, transform\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms, utils\n","from torch.autograd import Variable\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","\n","sns.set_context('notebook')\n","sns.set_style('white')"]},{"cell_type":"markdown","id":"e80ae0dd-81f5-4283-a936-c18e03955c80","metadata":{},"source":["### Defining Helper Functions\n"]},{"cell_type":"code","execution_count":3,"id":"5d15b829-6e1d-4761-a862-0aaf0e2525f4","metadata":{},"outputs":[],"source":["def f(modules_dict, x):\n","\n","    \"\"\"\n","    This function takes in an OrderedDict that contains all the modules of a defined RSU block and an input x\n","    and returns and result of the forward passing of the modules\n","    \n","    \"\"\"\n","    inner_enc_layers = []\n","    inner_dec_layers = []\n","    for layer_name, layer in modules_dict.items():\n","        if layer_name[-1] == 'd':\n","            inner_dec_layers.append(layer)\n","        else:\n","            inner_enc_layers.append(layer)\n","\n","    hx = x\n","    hxin = inner_enc_layers[0](hx)\n","    hx = hxin\n","    inner_enc_outputs = []\n","\n","    # inner block encode\n","    for layer in inner_enc_layers[1:]:\n","        hx = layer(hx)\n","        if not isinstance(layer, torch.nn.modules.pooling.MaxPool2d):\n","            inner_enc_outputs.append(hx)\n","\n","    # inner block decode + residual function\n","    hxd = inner_dec_layers[0](torch.cat((inner_enc_outputs.pop(), inner_enc_outputs.pop()),1))\n","    for layer in inner_dec_layers[1:]:\n","        hxdup = _upsample_like(hxd, inner_enc_outputs[-1])\n","        hxd = layer(torch.cat((hxdup, inner_enc_outputs.pop()),1))\n","\n","    return hxd + hxin"]},{"cell_type":"code","execution_count":4,"id":"03b63cd3-71d4-4854-804c-772513c6a461","metadata":{},"outputs":[],"source":["def normPRED(d):\n","    \n","    \"\"\"\n","    This function normalizes a prediction\n","    \n","    \"\"\"\n","    \n","    ma = torch.max(d)\n","    mi = torch.min(d)\n","\n","    dn = (d-mi)/(ma-mi)\n","\n","    return dn"]},{"cell_type":"markdown","id":"654e1c7a-ca6f-4051-8785-73167ff7b033","metadata":{},"source":["The following cell defines the `RescaleT` and `ToTensor` class, which will be helpful for tensor data transformation in a later section of this project.\n"]},{"cell_type":"code","execution_count":5,"id":"ebdd038a-6218-4258-b136-f3aa6df5909a","metadata":{},"outputs":[],"source":["class RescaleT(object):\n","    def __init__(self, output_size):\n","        self.output_size = output_size\n","    \n","    def __call__(self, sample):\n","        imidx, image = sample['imidx'], sample['image']\n","        img = transform.resize(image, (self.output_size, self.output_size), mode='constant')\n","        \n","        return {'imidx':imidx, 'image':img}\n","\n","    \n","class ToTensor(object):\n","    def __init__(self, flag=0):\n","        self.flag = flag\n","    \n","    def __call__(self, sample):\n","        imidx, image = sample['imidx'], sample['image']\n","        tmpImg = np.zeros((image.shape[0], image.shape[1], 3))\n","        image = image/np.max(image)\n","        \n","        tmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n","        tmpImg[:,:,1] = (image[:,:,1]-0.456)/0.224\n","        tmpImg[:,:,2] = (image[:,:,2]-0.406)/0.225\n","        \n","        tmpImg = tmpImg.transpose((2,0,1))\n","        \n","        return {'imidx':torch.from_numpy(imidx), 'image':torch.from_numpy(tmpImg)}"]},{"cell_type":"markdown","id":"5104f8a1-1f4a-46a7-871f-12a5a7078b34","metadata":{},"source":["## Background (optional)\n","\n","Segmenting the most visually attractive objects in a given image has been one of the most well-known tasks in the field of computer vision. It has wide applications in many fields, like **visual tracking** and **image segmentation**. You might have seen or played with autoencoders or other deep learning models such as [Deeplab](http://liangchiehchen.com/projects/DeepLab.htmlhttp://liangchiehchen.com/projects/DeepLab.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX0JV6EN1119-2022-01-01) for semantic segmentation. Here is an illustration of image segmentation:\n"]},{"cell_type":"markdown","id":"3d8ad86e-4c34-48c0-98c9-4a86ca9966c0","metadata":{},"source":["<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0JV6EN/images/semantic_seg.png\" width=\"65%\"></center>\n"]},{"cell_type":"markdown","id":"dc18c890-4287-4846-af8c-b90b720dfb6c","metadata":{},"source":["In the ocean of all the deep learning models, there is one name that still remains at the top, which is called U-Net. U-Net was released in 2018 and it has gained huge popularity since then for solving several different tasks related to segmentation. Below is a diagram showing the architecture of U-Net. \n"]},{"cell_type":"markdown","id":"9c01b1da-d7fa-48da-b469-41ee989a4b6c","metadata":{},"source":["<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0JV6EN/images/u-net-architecture.png\" width=\"65%\"></center>\n"]},{"cell_type":"markdown","id":"de1bfb67-0e15-47a4-a2c9-a5c674a7e4c5","metadata":{},"source":["<p style=\"color:gray; text-align:center;\">Image credits to <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX0JV6EN1119-2022-01-01\">Olaf Ronneberger et al.</a></p>\n"]},{"cell_type":"markdown","id":"316434c1-541d-4d5e-9d3c-bbd84d37c50c","metadata":{},"source":["You can find more information and the original paper of U-Net [here](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX0JV6EN1119-2022-01-01).\n"]},{"cell_type":"markdown","id":"9f19ac67-6c74-4604-872c-1b8a24d0b468","metadata":{},"source":["The task of detecting the most important or the **main object** in a given image is also called **Salient Object Detection** or **SOD**. With the development of deep Convolutional Neural Networks (CNNs), SOD has been significantly improved. \n"]},{"cell_type":"markdown","id":"33708b80-7129-4893-ad57-336a1e13f353","metadata":{},"source":["<p style='color: blue'>What is still missing?</p>\n","\n","\n","A common pattern in the design of most SOD networks is that they focus on using deep features extracted by existing model architectures, such as VGG, ResNet, DenseNet, etc. However, these architectures were originally designed and trained for image classification, which will not be suitable for SOD tasks that require **local details** and **global contrast information**.\n","\n","We are going to talk about a variant of U-Net called **U-squared Net** or **U2-Net**. It is basically a U-Net made of U-Net or a **two-level nested U-structure**, and was designed specifically for SOD tasks.\n","\n","Before we dive into the applications of U2-Net, let's first look at its key developments:\n","\n","1.  It can capture more contextual information, both locally and globally, from different scales thanks to the mixture of receptive fields of different sizes in the proposed **Residual U-blocks (RSUs)**.\n","\n","2.  It increases the depth of the architecture while maintaining high resolution feature maps at a low memory and computation cost, because of the dilated convolutions and pooling operations in the RSU blocks.\n"]},{"cell_type":"markdown","id":"b13dadac-2083-464d-bef2-ab33bef9c040","metadata":{},"source":["### What does a RSU block consist of?\n"]},{"cell_type":"markdown","id":"9057a610-09d2-40e8-9583-eb00471091ad","metadata":{},"source":["- An input convolution layer, which transforms the input feature map $x$ $(H \\times W \\times C_{in})$ to an intermediate map $F1(x)$ with the channel of $C_{out}$. This is a plain convolutional layer for **local feature extraction**.\n","\n","- A U-Net-like symmetric encoder-decoder structure with a height of $L$ that takes the intermediate feature map $F1(x)$ as input and learns to **extract and encode the multi-scale contextual information $U(F1(x))$**. Larger $L$ leads to deeper residual U-block (RSU), more pooling operations, a larger range of receptve fields, and richer local and global features. Configuring $L$ enables the extraction of multi-scale features from input feature maps with arbitrary spatial resolutions. **The multi-scale features are extracted from gradually downsampled feature maps and encoded into high resolution feature maps by progressively upsampling, concatenation and convolution.** This process mitigates the loss of fine details caused by direct upsampling with large scales.\n","\n","- A residual connection that **fuses local features and multi-scale features** by the summation $ F1(x) + U(F1(x))$.\n"]},{"cell_type":"markdown","id":"e4c21dfd-61bb-4268-91a6-27d18e258855","metadata":{},"source":["The following diagram illustrates the structure of a RSU block with $L=7$, where\n","\n","- Green box represents a _Convolution+BatchNorm+ReLU_ layer;\n","\n","- Blue box represents a **downsampling** _Convolution+BatchNorm+ReLU_ layer;\n","\n","- Pink box represents an **upsampling** _Convolution+BatchNorm+ReLU_ layer.\n"]},{"cell_type":"markdown","id":"84025b5c-2f5c-4b6b-8b2e-eb7b7d768773","metadata":{},"source":["<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0JV6EN/images/RSU.png\" width=\"40%\"></center>\n"]},{"cell_type":"markdown","id":"e8249005-5ef0-4520-904e-9794c03da698","metadata":{},"source":["<p style=\"color:gray; text-align:center;\">Image credits to <a href=\"https://github.com/xuebinqin/U-2-Net\">Xuebin Qin et al.</a></p>\n"]},{"cell_type":"markdown","id":"1b427029-c64b-4d9f-8dfd-363bba4741cf","metadata":{},"source":["Enough for the explanations so far. Let's do some coding!\n"]},{"cell_type":"markdown","id":"73154fcc-fa17-4c8c-9b5e-80f988470d02","metadata":{},"source":["## Code the RSU Blocks\n"]},{"cell_type":"markdown","id":"686e39ab-cecb-4fea-8758-2e5b15ec6377","metadata":{},"source":["As you saw, a single RSU block consists of many _Convolution+BatchNorm+ReLU_ layers. We can create a class called `REBNCONV` that bundles the Convolution, BatchNorm, and ReLU activation layers together, so that these layers can be added to our RSU block more easily and clearly.\n"]},{"cell_type":"code","execution_count":6,"id":"6173603b-3d20-414d-9645-d2f4e6dbe959","metadata":{},"outputs":[],"source":["class REBNCONV(nn.Module):\n","    def __init__(self, in_ch=3, out_ch=3, dirate=1):\n","        super(REBNCONV, self).__init__()\n","        \n","        self.conv_s1 = nn.Conv2d(in_ch,out_ch,3,padding=1*dirate,dilation=1*dirate)\n","        self.bn_s1 = nn.BatchNorm2d(out_ch)\n","        self.relu_s1 = nn.ReLU(inplace=True)\n","\n","        \n","    def forward(self, x):\n","        hx = x\n","        return self.relu_s1(self.bn_s1(self.conv_s1(hx)))"]},{"cell_type":"markdown","id":"b6c94094-e848-4719-af9a-2d40fdcaf5c7","metadata":{},"source":["Next, we define a function `_upsample_like` that upsamples `src` so that `src` has the same shape as `tar`. This will help with our concatenation of different shaped outputs later.\n"]},{"cell_type":"code","execution_count":7,"id":"c2697715-53a5-4810-a5ba-ed36061a2a3e","metadata":{},"outputs":[],"source":["def _upsample_like(src, tar):\n","    \n","    src = F.upsample(src, size=tar.shape[2:], mode='bilinear')\n","    return src"]},{"cell_type":"markdown","id":"dcae38f7-1a10-4239-903d-66263c5e474a","metadata":{},"source":["With these two helpers, let's start creating the RSU blocks! \n","\n","We will create several RSU blocks with different depths. All of them will be necessary for building our U2-Net architecture. \n","\n","<p style=\"color:blue;\">For feature maps with large height and width, we need greater $L$ to capture more large scale information.</P>\n","\n","I've simplified the forward functions of all the RSU blocks using the helper function `f`. If you are interested in knowing in detail how the layer outputs are forwarded in the RSU blocks, feel free to look at the code in the **Defining Helper Functions** section.\n"]},{"cell_type":"markdown","id":"d978a351-cdf5-44f2-a6da-405296342c0f","metadata":{},"source":["### RSU-7\n"]},{"cell_type":"markdown","id":"8c29b00b-a503-4c2d-9f9c-de6a10940583","metadata":{},"source":["A RSU-7 block has depth $(L)$ of 7. It consists of:\n","\n","- **One** input `REBNCONV` layer;\n","\n","- **Seven** downsamling `REBNCONV` layers, each followed by a `MaxPooling` layer except for `rebnconv6` and `rebnconv7` (to prevent the resolution of the last two layers being too low);\n","\n","- **six** upsampling `REBNCONV` layers, each takes the concatenation of the upsampled feature maps from its previous layer and those from its symmetrical upsamling layer as input.\n"]},{"cell_type":"code","execution_count":8,"id":"d7a11838-a019-499a-b6eb-42e15884c30c","metadata":{},"outputs":[],"source":["class RSU7(nn.Module):\n","\n","    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n","        super(RSU7,self).__init__()\n","\n","        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n","\n","        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n","        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","        self.pool5 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","\n","        self.rebnconv7 = REBNCONV(mid_ch,mid_ch,dirate=2)\n","\n","        self.rebnconv6d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n","\n","        \n","    def forward(self,x):\n","        \n","        modules_dict = self.__dict__['_modules']\n","        result = f(modules_dict, x)\n","        return result"]},{"cell_type":"markdown","id":"a6c95546-3739-4c71-96dd-4eeebf8e5c0d","metadata":{},"source":["### RSU-6\n"]},{"cell_type":"markdown","id":"e41f9285-fe8c-4569-86b2-51d14f472f25","metadata":{},"source":["A RSU-6 block has depth $(L)$ of 6. It consists of:\n","\n","- **One** input `REBNCONV` layer;\n","\n","- **Six** downsamling `REBNCONV` layers, each followed by a `MaxPooling` layer except for `rebnconv5` and `rebnconv6` (to prevent the resolution of the last two layers being too low);\n","\n","- **Five** upsampling `REBNCONV` layers, each takes the concatenation of the upsampled feature maps from its previous layer and those from its symmetrical upsamling layer as input.\n"]},{"cell_type":"code","execution_count":9,"id":"c28abbe1-f27d-461e-beb5-aa0d437216dc","metadata":{},"outputs":[],"source":["class RSU6(nn.Module):\n","\n","    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n","        super(RSU6,self).__init__()\n","\n","        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n","\n","        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n","        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","\n","        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=2)\n","\n","        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n","\n","        \n","    def forward(self,x):\n","        \n","        modules_dict = self.__dict__['_modules']\n","        result = f(modules_dict, x)\n","        return result\n","    "]},{"cell_type":"markdown","id":"332c8f20-a13c-43f0-a5c8-edcc91491965","metadata":{},"source":["### RSU-5\n"]},{"cell_type":"markdown","id":"550f715f-3aa1-47ab-95ba-4fe922d1570c","metadata":{},"source":["A RSU-5 block has depth $(L)$ of 5. It consists of:\n","\n","- **One** input `REBNCONV` layer;\n","\n","- **Five** downsamling `REBNCONV` layers, each followed by a `MaxPooling` layer except for `rebnconv4` and `rebnconv5` (to prevent the resolution of the last two layers being too low);\n","\n","- **Four** upsampling `REBNCONV` layers, each takes the concatenation of the upsampled feature maps from its previous layer and those from its symmetrical upsamling layer as input.\n"]},{"cell_type":"code","execution_count":10,"id":"77460a23-7556-491b-8523-ee78e4ca48b8","metadata":{},"outputs":[],"source":["class RSU5(nn.Module):\n","\n","    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n","        super(RSU5,self).__init__()\n","\n","        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n","\n","        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n","        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","\n","        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=2)\n","\n","        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n","        \n","    \n","    def forward(self,x):\n","        \n","        modules_dict = self.__dict__['_modules']\n","        result = f(modules_dict, x)\n","        return result"]},{"cell_type":"markdown","id":"5b5c1f6e-e4b2-48f2-87ac-93f7072eabd1","metadata":{},"source":["### Exercise: Create RSU-4\n","\n","As an exercise, please code the RSU-4 block yourself based on the prompt below.\n"]},{"cell_type":"markdown","id":"06431b64-89cc-489d-b093-bfef6dfe4b37","metadata":{},"source":["A RSU-4 block has depth $(L)$ of 4. It consists of:\n","\n","- **One** input `REBNCONV` layer;\n","\n","- **Four** downsamling `REBNCONV` layers, each followed by a `MaxPooling` layer except for `rebnconv3` and `rebnconv4` (to prevent the resolution of the last two layers being too low);\n","\n","- **Three** upsampling `REBNCONV` layers, each takes the concatenation of the upsampled feature maps from its previous layer and those from its symmetrical upsamling layer as input.\n"]},{"cell_type":"code","execution_count":11,"id":"80466bfa-207f-4846-b0cb-99cc4d6bbb87","metadata":{},"outputs":[],"source":["class RSU4(nn.Module):\n","\n","    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n","        super(RSU4,self).__init__()\n","\n","        # your code here\n","        \n","\n","        \n","    def forward(self,x):\n","        \n","        modules_dict = self.__dict__['_modules']\n","        result = f(modules_dict, x)\n","        return result\n","        "]},{"cell_type":"markdown","id":"c7debdac-c5fa-426a-8f56-99bacc399d14","metadata":{},"source":["<details><summary>Click Here for Solution</summary>\n","    \n","```python\n","\n","self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n","\n","self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n","self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n","\n","self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=2)\n","\n","self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n","self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n","\n","```\n","</details>\n"]},{"cell_type":"markdown","id":"a27c910e-baba-410a-93d4-d9a15c384d99","metadata":{},"source":["### RSU-4F\n"]},{"cell_type":"markdown","id":"15a23a92-6024-431a-af64-47b9befb738d","metadata":{},"source":["The RSU-4F block is slightly different from other RSU-L blocks. **\"F\"** means that the RSU is a dilated version, where we replace the pooling the upsampling operations with dilated convolutions. This means all of intermediate feature maps of RSU-4F have the same resolution with its input feature maps.\n","\n","A RSU-4F has depth $(L)$ of 4. It consists of:\n","\n","- **One** input `REBNCONV` layer;\n","\n","- **Four** dilated `REBNCONV` layers, each produces output of the same shape as the input.\n","\n","- **Three** dilated `REBNCONV` layers, each takes the concatenation of the feature maps from its previous layer and those from its symmetrical layer as input.\n"]},{"cell_type":"code","execution_count":12,"id":"35766f5f-c656-4499-9798-ae7b5f574c50","metadata":{},"outputs":[],"source":["class RSU4F(nn.Module):\n","    \n","    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n","        super(RSU4F,self).__init__()\n","\n","        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n","\n","        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n","        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=2)\n","        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=4)\n","\n","        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=8)\n","\n","        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=4)\n","        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=2)\n","        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n","\n","        \n","    def forward(self,x):\n","        \n","        modules_dict = self.__dict__['_modules']\n","        result = f(modules_dict, x)\n","        return result"]},{"cell_type":"markdown","id":"a424c308-a2ed-4bb1-99d4-862b62cf5c25","metadata":{},"source":["We have finished creating all the RSU blocks that are required for the complete U2-Net architecture! \n","\n","We can print out the `named_parameters` of a RSU block to see the parameters associated with each of the layers we included in the block.\n"]},{"cell_type":"code","execution_count":13,"id":"4bcb6701-41cd-44af-8a9b-79a531198f89","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["rebnconvin.conv_s1.weight\n","rebnconvin.conv_s1.bias\n","rebnconvin.bn_s1.weight\n","rebnconvin.bn_s1.bias\n","rebnconv1.conv_s1.weight\n","rebnconv1.conv_s1.bias\n","rebnconv1.bn_s1.weight\n","rebnconv1.bn_s1.bias\n","rebnconv2.conv_s1.weight\n","rebnconv2.conv_s1.bias\n","rebnconv2.bn_s1.weight\n","rebnconv2.bn_s1.bias\n","rebnconv3.conv_s1.weight\n","rebnconv3.conv_s1.bias\n","rebnconv3.bn_s1.weight\n","rebnconv3.bn_s1.bias\n","rebnconv4.conv_s1.weight\n","rebnconv4.conv_s1.bias\n","rebnconv4.bn_s1.weight\n","rebnconv4.bn_s1.bias\n","rebnconv3d.conv_s1.weight\n","rebnconv3d.conv_s1.bias\n","rebnconv3d.bn_s1.weight\n","rebnconv3d.bn_s1.bias\n","rebnconv2d.conv_s1.weight\n","rebnconv2d.conv_s1.bias\n","rebnconv2d.bn_s1.weight\n","rebnconv2d.bn_s1.bias\n","rebnconv1d.conv_s1.weight\n","rebnconv1d.conv_s1.bias\n","rebnconv1d.bn_s1.weight\n","rebnconv1d.bn_s1.bias\n"]}],"source":["for name, param in RSU4F().named_parameters():\n","    print(name)"]},{"cell_type":"markdown","id":"a1d4314e-8536-4ea6-94c4-60d3a4cf79bc","metadata":{},"source":["You can see that the parameters are associated with the Convolution and the BatchNorm layer of `REBNCONV`.\n"]},{"cell_type":"markdown","id":"6ba4e53b-6164-4394-a133-e5a5f37dbde2","metadata":{},"source":["## U2-Net Architecture: Two-level Nested U-structure\n"]},{"cell_type":"markdown","id":"e03bdaec-4830-4ed7-9c2f-3ce1cda2d6fa","metadata":{},"source":["The U2-Net is a **two-level nested U-structure**. As shown in the figure below, its top level is a big U-structure consists of 11 stages where each stage is a RSU block (bottom level U-structure). Thus, the nested U-structure enables the **extraction of intra-stage multi-scale features** and **aggregation of inner-stage multi-level features**.\n"]},{"cell_type":"markdown","id":"a78d9ae4-0e0e-4499-a67f-cb12b104dc89","metadata":{},"source":["<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0JV6EN/images/U2Net.png\" width=\"70%\"></center>\n"]},{"cell_type":"markdown","id":"d4af78c5-e167-4dac-baba-92339796984e","metadata":{},"source":["<p style=\"color:gray; text-align:center;\">Image credits to <a href=\"https://github.com/xuebinqin/U-2-Net\">Xuebin Qin et al.</a></p>\n"]},{"cell_type":"markdown","id":"adb19d64-214f-4dfd-82a0-a2ea1247f7b0","metadata":{},"source":["### Configure U2-Net\n"]},{"cell_type":"markdown","id":"e9103de3-c7c8-40c8-82d5-49055b87f2dc","metadata":{},"source":["Using all the RSU-L blocks and the RSU-4F block we created previously, let's configure the U2-Net. In a nutshell, the U2-Net mainly consists of three parts:\n","\n","- **a six stages encoder** composed of RSU-7, RSU-6, RSU-5, RSU-4, RSU-4F, and RSU-4F;\n","\n","- **a five stages decoder** composed of RSU-4F, RSU-4, RSU-5, RSU-6, and RSU-7;  \n","\n","- **a saliency map fusion module** attached with the decoder stages and the last encoder stage for generating side output saliency probability maps; they will be fused to produce the final saliency probability map.\n","\n","<p style=\"color:blue\">Each decoder stage takes the concatenation of the decoded feature maps from its previous stage and those from its symmetrical encoder stage as the input.</p>\n"]},{"cell_type":"code","execution_count":14,"id":"7e7bd230-735f-4749-ae05-3f151c5a8ef3","metadata":{},"outputs":[],"source":["class U2NET(nn.Module):\n","\n","    def __init__(self,in_ch=3,out_ch=1):\n","        super(U2NET,self).__init__()\n","\n","        # encoder\n","        self.stage1 = RSU7(in_ch,32,64)\n","        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.stage2 = RSU6(64,32,128)\n","        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.stage3 = RSU5(128,64,256)\n","        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.stage4 = RSU4(256,128,512)\n","        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.stage5 = RSU4F(512,256,512)\n","        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n","\n","        self.stage6 = RSU4F(512,256,512)\n","\n","        # decoder\n","        self.stage5d = RSU4F(1024,256,512)\n","        self.stage4d = RSU4(1024,128,256)\n","        self.stage3d = RSU5(512,64,128)\n","        self.stage2d = RSU6(256,32,64)\n","        self.stage1d = RSU7(128,16,64)\n","\n","        #side \n","        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n","        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n","        self.side3 = nn.Conv2d(128,out_ch,3,padding=1)\n","        self.side4 = nn.Conv2d(256,out_ch,3,padding=1)\n","        self.side5 = nn.Conv2d(512,out_ch,3,padding=1)\n","        self.side6 = nn.Conv2d(512,out_ch,3,padding=1)\n","\n","        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n","        \n","        \n","    def forward(self,x):\n","\n","        hx = x\n","\n","        #stage 1\n","        hx1 = self.stage1(hx)\n","        hx = self.pool12(hx1)\n","\n","        #stage 2\n","        hx2 = self.stage2(hx)\n","        hx = self.pool23(hx2)\n","\n","        #stage 3\n","        hx3 = self.stage3(hx)\n","        hx = self.pool34(hx3)\n","\n","        #stage 4\n","        hx4 = self.stage4(hx)\n","        hx = self.pool45(hx4)\n","\n","        #stage 5\n","        hx5 = self.stage5(hx)\n","        hx = self.pool56(hx5)\n","\n","        #stage 6\n","        hx6 = self.stage6(hx)\n","        hx6up = _upsample_like(hx6,hx5)\n","\n","        #-------------------- decoder --------------------\n","        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n","        hx5dup = _upsample_like(hx5d,hx4)\n","\n","        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n","        hx4dup = _upsample_like(hx4d,hx3)\n","\n","        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n","        hx3dup = _upsample_like(hx3d,hx2)\n","\n","        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n","        hx2dup = _upsample_like(hx2d,hx1)\n","\n","        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n","\n","\n","        #side output\n","        d1 = self.side1(hx1d)\n","\n","        d2 = self.side2(hx2d)\n","        d2 = _upsample_like(d2,d1)\n","\n","        d3 = self.side3(hx3d)\n","        d3 = _upsample_like(d3,d1)\n","\n","        d4 = self.side4(hx4d)\n","        d4 = _upsample_like(d4,d1)\n","\n","        d5 = self.side5(hx5d)\n","        d5 = _upsample_like(d5,d1)\n","\n","        d6 = self.side6(hx6)\n","        d6 = _upsample_like(d6,d1)\n","\n","        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n","\n","        return F.sigmoid(d0) #, F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)\n"]},{"cell_type":"markdown","id":"31a77d2b-98de-461f-b5f9-4c8050ef6654","metadata":{},"source":["The forward function of U2-Net returns the final saliency probability map $sigmoid(d_0)$ that's generated using the side outputs $d_1,d_2, d_3, d_4, d_5, d_6$. Depending on the application of U2-Net, the final saliency probability map can be used differently to produce the final product desired.\n"]},{"cell_type":"markdown","id":"fd7e7544-ba16-4a6e-ada5-657d6ad8a597","metadata":{},"source":["## Use U2-Net to Draw Human Portrait\n"]},{"cell_type":"markdown","id":"7d3dd740-a6a1-4abc-84f6-6aa5307b6211","metadata":{},"source":["One of the interesting applications of U2-Net is human portrait generation. In this section, we will import the pre-trained weights of the U2-Net and load them into our model architecture. \n","\n","### Training \n","\n","If you are interested in training the U2-Net yourself (in your local environment using GPUs or in [IBM Watson Studio](https://cloud.ibm.com/catalog/services/watson-studio?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX0JV6EN1119-2022-01-01)), you can download our [u2net_training_notebook](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0JV6EN/data/u2net_training_notebook.ipynb) which borrows the training methods and training datasets in this [repository](https://github.com/xuebinqin/U-2-Net) and run the notebook in a different environment.\n"]},{"cell_type":"markdown","id":"73c27a34-c115-49ba-895d-375f574688e0","metadata":{},"source":["### Pre-trained U-squared Net\n"]},{"cell_type":"markdown","id":"7a78b31b-5f13-4585-b344-381e336d2123","metadata":{},"source":["We create a U2-Net model using our defined class `U2NET`and call it `net`.\n"]},{"cell_type":"code","execution_count":15,"id":"9b925577-3df5-4217-812b-b465c1734f86","metadata":{},"outputs":[],"source":["net = U2NET(3, 1)"]},{"cell_type":"markdown","id":"720aa05a-d421-4401-b914-8cd81a05547d","metadata":{},"source":["Downloading the model weights and load them into `net`:\n"]},{"cell_type":"code","execution_count":17,"id":"1422303c-acb2-4acf-a6a7-5759e12cd8e7","metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '\\\\tmp\\\\skills-network-42188626218589019-u2net.tgz'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m skillsnetwork\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0JV6EN/LargeData/u2net.tgz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pretrained_u2net\u001b[39m\u001b[38;5;124m\"\u001b[39m, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_u2net/u2net.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","File \u001b[1;32mc:\\Users\\Kaleem KM\\miniconda3\\envs\\tf_env\\lib\\site-packages\\skillsnetwork\\core.py:249\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(url, path, verbose, overwrite)\u001b[0m\n\u001b[0;32m    246\u001b[0m tmp_download_file \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Download the dataset to tmp_download_file file\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# File will be overwritten if it already exists\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m download(url, tmp_download_file, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Delete extract_dir directory if it already exists\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_jupyterlite():\n","File \u001b[1;32mc:\\Users\\Kaleem KM\\miniconda3\\envs\\tf_env\\lib\\site-packages\\skillsnetwork\\core.py:186\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, path, verbose, chunk_size)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m    185\u001b[0m         path \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# Will raise FileNotFoundError if invalid path\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m _get_chunks(url, chunk_size):\n\u001b[0;32m    188\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\tmp\\\\skills-network-42188626218589019-u2net.tgz'"]}],"source":["await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0JV6EN/LargeData/u2net.tgz\", \"./pretrained_u2net\", overwrite=True)\n","\n","net.load_state_dict(torch.load(\"pretrained_u2net/u2net.pth\", map_location='cpu'))"]},{"cell_type":"markdown","id":"a51c4a60-d568-41c3-97b9-8b0df05cfb41","metadata":{},"source":["Once you see the \"All keys matched successfully\" statement, you know that your U2-Net model is ready for some inference!\n"]},{"cell_type":"markdown","id":"5d28a42e-6601-4549-bcd7-73c37009e32f","metadata":{},"source":["To use the U2-Net for human portrait generation, let's first prepare some test photos of human faces.\n"]},{"cell_type":"code","execution_count":null,"id":"b7aea64a-a5e9-4698-833a-b810e9439784","metadata":{},"outputs":[],"source":["await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0JV6EN/data/test.tgz\", overwrite=True)"]},{"cell_type":"markdown","id":"37fcab76-5db7-4c6d-a70f-0f5afc9ebf41","metadata":{},"source":["### Creating a Dataloader\n"]},{"cell_type":"markdown","id":"c9886c94-07b5-41f3-b3a2-f52c5dee312b","metadata":{},"source":["We need to create a PyTorch dataloader for feeding the test data to `net`. The `LoadDataset` class can produce a PyTorch Dataset object that generates samples of test images.\n"]},{"cell_type":"code","execution_count":null,"id":"eef22853-d92a-47d8-9d9a-59e2be41b506","metadata":{},"outputs":[],"source":["class LoadDataset(Dataset):\n","    def __init__(self, img_dir, transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.img_name_list = [x for x in os.listdir(img_dir) if not x.startswith(\".\")]\n","        \n","        \n","    def __len__(self):\n","        return len(self.img_name_list)\n","    \n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","            \n","        img_name = os.path.join(self.img_dir, self.img_name_list[idx])\n","        image = io.imread(img_name)\n","        imidx = np.array([idx])\n","        sample = {'imidx':imidx, 'image':image}\n","        \n","        if self.transform:\n","            sample = self.transform(sample)\n","            \n","        return sample    \n"]},{"cell_type":"markdown","id":"78ebea40-214e-43b9-bac9-6e0a198bf8c2","metadata":{},"source":["As the test photos were downloaded to the `images` folder, we pass the path to `LoadDataset`. We also specify that we want to transform the images using `RescaleT` and `ToTensor`, which were defined previously in the **Defining Helper Functions** section.\n"]},{"cell_type":"code","execution_count":null,"id":"7a445a33-656a-4ffc-ad8a-2708400a2cc3","metadata":{},"outputs":[],"source":["photos_dataset = LoadDataset(img_dir = './images/test_photos', \n","                             transform = transforms.Compose([RescaleT(512),\n","                                                             ToTensor(0)]))"]},{"cell_type":"markdown","id":"23abaedc-a8fb-482a-915e-808fd195b807","metadata":{},"source":["We create a PyTorch dataloader called `photos_dataloader` with `photos_dataset`. Batch size equals 1 because we will inference one photo at a time.\n"]},{"cell_type":"code","execution_count":null,"id":"73dad0ee-3db3-4b1e-a82b-d8bab4a43c5f","metadata":{},"outputs":[],"source":["photos_dataloader = DataLoader(photos_dataset,\n","                               batch_size=1)"]},{"cell_type":"markdown","id":"58617e32-2e2d-4b88-94f5-da893696a519","metadata":{},"source":["Let's visualize the photos in our `photos_dataloader`:\n"]},{"cell_type":"code","execution_count":null,"id":"45c69ff2-928d-44e0-addf-14e73415ce6a","metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(10,4))\n","\n","for i, sample in enumerate(photos_dataloader):\n","    img_arr = np.transpose((sample['image'].numpy()[0]+3)/6, (1,2,0))\n","    ax = fig.add_subplot(2,5,i+1)\n","    ax.imshow(img_arr)\n","    ax.axis(\"off\")"]},{"cell_type":"markdown","id":"9f54fe42-da2f-4724-a79b-3003d7cb0a5c","metadata":{},"source":["We have photos of 10 human faces, which we will now pass to our U2-Net `net` for generating portraits of these faces.\n"]},{"cell_type":"markdown","id":"9ff115cc-c641-48b2-b623-e83c1d6f90bf","metadata":{},"source":["### Inference!\n"]},{"cell_type":"markdown","id":"a36c61fe-0b62-4b4f-ba84-d67b81ae78aa","metadata":{},"source":["The code below takes care of the inference procedure. For each input image, it takes the saliency probability map predicted by the U2-Net, normalizes the prediction, and converts it back to a RGB image for visualization. It should take less than 5 minutes to finish inferencing 10 images. \n"]},{"cell_type":"code","execution_count":null,"id":"c83fbafb-3f30-473c-87f6-f5f0db285538","metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(10,4))\n","\n","for i, sample in enumerate(photos_dataloader):\n","\n","\n","    print(f\"inferencing photo #{i+1}:\")\n","\n","    input_test = sample['image']\n","    input_test = input_test.type(torch.FloatTensor)\n","    input_test = Variable(input_test)\n","\n","    d = net(input_test)\n","\n","    # normalization\n","    pred = 1.0 - d[:,0,:,:]\n","    pred = normPRED(pred)\n","\n","    predict = pred.squeeze()\n","    predict_np = predict.cpu().data.numpy()\n","    img = Image.fromarray(predict_np*255).convert('RGB')\n","    img_array = np.array(img)\n","\n","    ax = fig.add_subplot(2,5,i+1)\n","    ax.imshow(img_array)\n","    ax.axis(\"off\")\n","\n","    del input_test, d, pred, predict, predict_np, img, img_array"]},{"cell_type":"markdown","id":"f1c8e58d-741f-4a14-86eb-3c88dd85df87","metadata":{},"source":["Here you go! Ten nicely generated human portraits by our U2-Net. \n"]},{"cell_type":"markdown","id":"9cc5b9a1-6737-4a01-b98e-7c155e721da2","metadata":{},"source":["### Exercise: Generate your own portrait\n"]},{"cell_type":"code","execution_count":null,"id":"f90d798e-99e1-4121-936a-f909120601eb","metadata":{},"outputs":[],"source":["! mkdir my_photos"]},{"cell_type":"markdown","id":"3c56764a-90b8-4bae-ad05-60e1c650101e","metadata":{},"source":["As an exercise, open the file browser and upload your own photos to the `my_photos` folder to get your portraits! \n"]},{"cell_type":"markdown","id":"95426631-a980-43b8-a661-18b4fcc9f925","metadata":{},"source":["**Step 1:** Upload your photos to `my_photos`.\n"]},{"cell_type":"markdown","id":"e2f9f309-9cdb-4723-b53d-7326cb525141","metadata":{},"source":["**Step 2:** Create a PyTorch dataloader for your photos.\n"]},{"cell_type":"code","execution_count":null,"id":"6a36fd39-426a-40f8-b22f-7efedd129db0","metadata":{},"outputs":[],"source":["# begin your code\n","\n","# your_dataset = \n","# your_dataloader ="]},{"cell_type":"markdown","id":"59bc52c1-50ac-4c6f-996a-495038c1a248","metadata":{},"source":["<details><summary>Click Here for Solution</summary>\n","    \n","```python\n","\n","your_dataset = LoadDataset(img_dir = './my_photos', \n","                             transform = transforms.Compose([RescaleT(512),\n","                                                             ToTensor(0)]))\n","your_dataloader = DataLoader(your_dataset,\n","                               batch_size=1)\n","\n","```\n","</details>\n"]},{"cell_type":"markdown","id":"d4538153-d43c-42ef-b141-8494e685eaf2","metadata":{},"source":["**Step 3:** Inference/Generate portrait(s) of your photo(s).\n"]},{"cell_type":"code","execution_count":null,"id":"2651e0e8-1ec7-4d3e-9758-a37605fe5b12","metadata":{},"outputs":[],"source":["for i, sample in enumerate(your_dataloader):\n","\n","\n","    print(f\"inferencing photo #{i+1}:\")\n","\n","    input_test = Variable(sample['image'].type(torch.FloatTensor))\n","    d = net(input_test)\n","\n","    # normalization\n","    pred = normPRED(1.0 - d[:,0,:,:])\n","\n","    predict = pred.squeeze().cpu().data.numpy()\n","    img = Image.fromarray(predict*255).convert('RGB')\n","\n","    plt.imshow(np.array(img))\n","    plt.axis(\"off\")\n","\n","    del input_test, d, pred, predict, img"]},{"cell_type":"markdown","id":"b826bd72-5e40-4df8-8e1a-b0c7d75d1223","metadata":{},"source":["## Congratulations on completing this guided project!\n"]},{"cell_type":"markdown","id":"5c422232-d777-4178-9b86-07cb1f3418d0","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","id":"9716dfd2-5c4e-4b7a-95b0-1a2ccf71db9b","metadata":{},"source":["[Roxanne Li](https://www.linkedin.com/in/roxanne-li/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsINDGPXX0JV6EN1119-2022-01-01) is a Data Science intern at IBM Skills Network and a current undergraduate Mathematics & Statistic student at McMaster University.\n"]},{"cell_type":"markdown","id":"9b4b3dca-d4f6-4d74-bc63-dc210506ef7a","metadata":{},"source":["## Change Log\n"]},{"cell_type":"markdown","id":"0ecbbf07-4114-489a-b363-17b45752bd10","metadata":{},"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2020-07-17|0.1|Sam|Create Lab Template|\n","|2020-11-02|0.1|Roxanne Li|Created guided project|\n"]},{"cell_type":"markdown","id":"c416cc61-b917-41f0-b5eb-42ad5a97b5b9","metadata":{},"source":["Copyright  2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"tf_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
